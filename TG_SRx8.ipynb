{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.datasets import CocoCaptions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Detection.GroundingDINO.groundingdino.datasets.transforms as T\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,phase, transform):\n",
    "        self.phase = phase\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.dataset = CocoCaptions(root='./train2017', annFile='./annotations/captions_train2017.json')\n",
    "        else:\n",
    "            self.dataset = CocoCaptions(root='./val2017', annFile='./annotations/captions_val2017.json')\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        org_img, caps = self.dataset[idx]\n",
    "\n",
    "        img,_ = self.transform(org_img, None)\n",
    "        \n",
    "        random_idx = torch.randint(0,len(caps), (1,))\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            return np.array(org_img), img, caps[random_idx]\n",
    "        else:\n",
    "            return np.array(org_img), img, caps[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SResolution.TG_Umodels import *\n",
    "\n",
    "def select_model(down_scale, shape):\n",
    "    if down_scale == 2:\n",
    "        model = TG_UNet2(3,3, shape)\n",
    "    elif down_scale == 4:\n",
    "        model = TG_UNet4(3,3, shape)\n",
    "    elif down_scale == 8:\n",
    "        model = TG_UNet8x2(3,3, shape)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "down_scale = 8\n",
    "input_shape = (512//down_scale,512//down_scale)\n",
    "model = select_model(down_scale,input_shape)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 30\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.64s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "\n",
    "trainDataset = CustomDataset('train', transform)\n",
    "\n",
    "num_train_samples = 1000\n",
    "sample_ds = Subset(trainDataset, np.arange(num_train_samples))\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "# sample_dl = DataLoader(sample_ds, sampler=sample_sampler, batch_size=1)\n",
    "train_dataloader = DataLoader(sample_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "valDataset = CustomDataset('val', transform)\n",
    "num_train_samples = 200\n",
    "sample_ds = Subset(valDataset, np.arange(num_train_samples))\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "val_dataloader = DataLoader(sample_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 480, 640, 3]), torch.Size([1, 3, 800, 1066]), tuple)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo, a, c = next(iter(train_dataloader))\n",
    "\n",
    "oo.shape, a.shape, type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Train:   0%|          | 0/1000 [00:00<?, ?it/s]FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "                                               \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "narrow(): length must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m     nh, nw \u001b[38;5;241m=\u001b[39m (target_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mdown_scale) \u001b[38;5;241m-\u001b[39m hh, (target_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mdown_scale) \u001b[38;5;241m-\u001b[39m ww\n\u001b[0;32m     73\u001b[0m     padd \u001b[38;5;241m=\u001b[39m (nw\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, nw\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, nh\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, nh\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m     simg \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhh\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mww\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     train_result[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(caps[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresized\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(s_img)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# for b in train_result[f'{str(caps[0])}']['padded']:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#     _, h, w = b.shape\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#     s_img = resize(b, (h//down_scale, w//down_scale))\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#     train_result[f'{str(caps[0])}']['resized'].append(s_img)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: narrow(): length must be non-negative."
     ]
    }
   ],
   "source": [
    "from Detection.GroundingDINO.groundingdino.util.inference import load_model, load_image_6, predict, annotate\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from torchvision.ops import box_convert\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "iterable = range(epochs)\n",
    "\n",
    "dect_model = load_model(\"./Detection/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"./Detection/GroundingDINO//weights/groundingdino_swint_ogc.pth\")\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "target_size = 512\n",
    "\n",
    "model.to(device)\n",
    "send_tg_message(f'Start Local Computer X8')\n",
    "best_loss = float(\"inf\") \n",
    "best_result = None\n",
    "for e in iterable:\n",
    "    for p in ['Train', 'Test']:\n",
    "        if p == 'Train':\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            model.train()\n",
    "\n",
    "            train_result = {}\n",
    "            \n",
    "            for oo, bimg, caps in tqdm(train_dataloader, desc='Train', leave=False):\n",
    "                # print(f'{str(caps[0])}')\n",
    "                train_result[f'{str(caps[0])}'] = {\n",
    "                    'phrases': [],\n",
    "                    'org_img':[],\n",
    "                    'shape':[],\n",
    "                    'cropped':[],\n",
    "                    'padded':[],\n",
    "                    'resized':[],\n",
    "                    'output':[],\n",
    "                    'c_padded':[]\n",
    "                }\n",
    "                \n",
    "                i_caps = str(caps[0])\n",
    "                bimg = bimg.squeeze()\n",
    "                b_boxes, b_logits, b_phrases, b_encoded_text = predict(\n",
    "                        model=dect_model,\n",
    "                        image=bimg,\n",
    "                        caption=i_caps,\n",
    "                        box_threshold=BOX_TRESHOLD,\n",
    "                        text_threshold=TEXT_TRESHOLD\n",
    "                    )\n",
    "                train_result[f'{str(caps[0])}']['phrases'].append(b_phrases)\n",
    "                oo = oo.squeeze()\n",
    "                h, w, c = oo.shape\n",
    "                boxes2 = b_boxes * torch.Tensor([w, h, w, h])\n",
    "                xyxy = box_convert(boxes=boxes2, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "                \n",
    "                for xyxy_idx in range(len(xyxy)):\n",
    "                    x1, y1, x2, y2 = xyxy[xyxy_idx].astype(int)\n",
    "                    cropped_img = oo[y1:y2, x1:x2].permute(2,1,0)\n",
    "\n",
    "                    cc, hh, ww = cropped_img.shape\n",
    "                    train_result[f'{str(caps[0])}']['shape'].append((hh,ww))\n",
    "                    nh, nw = target_size - hh, target_size - ww\n",
    "\n",
    "                    train_result[f'{str(caps[0])}']['cropped'].append(cropped_img)\n",
    "                    padd = (nw//2, nw//2, nh//2, nh//2)\n",
    "                    pimg = torch.nn.functional.pad(cropped_img, padd)\n",
    "                    pimg = resize(pimg, (target_size, target_size))\n",
    "                    train_result[f'{str(caps[0])}']['padded'].append(pimg)\n",
    "\n",
    "                    nh, nw = target_size - hh, target_size - ww\n",
    "                \n",
    "                for b in train_result[f'{str(caps[0])}']['padded']:\n",
    "                    _, h, w = b.shape\n",
    "                    s_img = resize(b, (h//down_scale, w//down_scale))\n",
    "                    train_result[f'{str(caps[0])}']['resized'].append(s_img)\n",
    "\n",
    "                txt = b_encoded_text['encoded_text']\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                for re, pa, sha in zip(train_result[f'{str(caps[0])}']['resized'], train_result[str(caps[0])]['cropped'],train_result[str(caps[0])]['shape'] ):\n",
    "                    re, pa = re/255., pa/255.\n",
    "                    # re, pa = re.float(), pa.float()\n",
    "                    \n",
    "                    re = re.unsqueeze(0)\n",
    "                    # pa = pa.unsqueeze(0)\n",
    "                    re = re.to(device)\n",
    "                    # pa = pa.to(device)\n",
    "\n",
    "                    pa = torch.tensor(pa).unsqueeze(0)\n",
    "                    pa = pa.to(device)\n",
    "\n",
    "                    output = model(re,txt)\n",
    "                    \n",
    "                    c_output = center_crop(output, sha)\n",
    "                    # c_pa = center_crop(pa, sha)\n",
    "                    # print(c_output.shape, pa.shape)\n",
    "\n",
    "                    train_result[f'{str(caps[0])}']['output'].append(c_output.detach().squeeze(0).cpu().numpy())\n",
    "                    # train_result[f'{str(caps[0])}']['c_padded'].append(c_pa.detach().squeeze(0).cpu().numpy())\n",
    "                    train_result[f'{str(caps[0])}']['c_padded'].append(pa.detach().squeeze(0).cpu().numpy())\n",
    "\n",
    "                    # loss = criterion(c_output, c_pa)\n",
    "                    loss = criterion(c_output, pa)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss\n",
    "                    total += 1\n",
    "            avg_loss = total_loss / total\n",
    "            train_text = f'Train | Epoch: [{e+1}/{epochs}] |  MSE: {avg_loss} | Total: {total}'\n",
    "            print('='*len(train_text))\n",
    "            print(train_text)\n",
    "\n",
    "        else:\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "\n",
    "            test_result = {}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for oo, bimg, caps in tqdm(val_dataloader, desc='Test', leave=False):\n",
    "\n",
    "                    test_result[f'{str(caps[0])}'] = {\n",
    "                        'phrases': [],\n",
    "                        'org_img':[],\n",
    "                        'shape':[],\n",
    "                        'cropped':[],\n",
    "                        'padded':[],\n",
    "                        'resized':[],\n",
    "                        'output':[],\n",
    "                        'c_padded':[],\n",
    "                        'detect':[]\n",
    "                    }\n",
    "\n",
    "                    i_caps = str(caps[0])\n",
    "                    bimg = bimg.squeeze()\n",
    "                    b_boxes, b_logits, b_phrases, b_encoded_text = predict(\n",
    "                            model=dect_model,\n",
    "                            image=bimg,\n",
    "                            caption=i_caps,\n",
    "                            box_threshold=BOX_TRESHOLD,\n",
    "                            text_threshold=TEXT_TRESHOLD\n",
    "                        )\n",
    "                    test_result[f'{str(caps[0])}']['phrases'].append(b_phrases)\n",
    "                    oo = oo.squeeze()\n",
    "                    h, w, c = oo.shape\n",
    "                    boxes2 = b_boxes * torch.Tensor([w, h, w, h])\n",
    "                    xyxy = box_convert(boxes=boxes2, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "                    test_result[f'{str(caps[0])}']['org_img'].append(oo)\n",
    "                    for xyxy_idx in range(len(xyxy)):\n",
    "                        x1, y1, x2, y2 = xyxy[xyxy_idx].astype(int)\n",
    "                        cropped_img = oo[y1:y2, x1:x2].permute(2,1,0)\n",
    "\n",
    "                        cc, hh, ww = cropped_img.shape\n",
    "                        \n",
    "                        test_result[f'{str(caps[0])}']['shape'].append((hh,ww))\n",
    "                        nh, nw = target_size - hh, target_size - ww\n",
    "\n",
    "                        test_result[f'{str(caps[0])}']['cropped'].append(cropped_img)\n",
    "                        padd = (nw//2, nw//2, nh//2, nh//2)\n",
    "                        pimg = torch.nn.functional.pad(cropped_img, padd)\n",
    "                        \n",
    "                        pimg = resize(pimg, (target_size, target_size))\n",
    "                        test_result[f'{str(caps[0])}']['padded'].append(pimg)\n",
    "\n",
    "                        test_result[f'{str(caps[0])}']['detect'].append(resize(cropped_img, (hh//down_scale, ww//down_scale)))\n",
    "\n",
    "                    for b in test_result[f'{str(caps[0])}']['padded']:\n",
    "                        _, h, w = b.shape\n",
    "                        s_img = resize(b, (h//down_scale, w//down_scale))\n",
    "                        \n",
    "                        test_result[f'{str(caps[0])}']['resized'].append(s_img)\n",
    "\n",
    "                    txt = b_encoded_text['encoded_text']\n",
    "\n",
    "                    for re, pa, sha in zip(test_result[f'{str(caps[0])}']['resized'], test_result[str(caps[0])]['cropped'], test_result[str(caps[0])]['shape']):\n",
    "                        re, pa = re/255., pa/255.\n",
    "                        # re, pa = re.float(), pa.float()\n",
    "                        # re = re.unsqueeze(0)\n",
    "                        # pa = pa.unsqueeze(0)\n",
    "                        # re = re.to(device)\n",
    "                        # pa = pa.to(device)\n",
    "                        re = re.unsqueeze(0)\n",
    "                        # pa = pa.unsqueeze(0)\n",
    "                        re = re.to(device)\n",
    "                        # pa = pa.to(device)\n",
    "\n",
    "                        pa = torch.tensor(pa).unsqueeze(0)\n",
    "                        pa = pa.to(device)\n",
    "\n",
    "                        output = model(re,txt)\n",
    "\n",
    "                        c_output = center_crop(output, sha)\n",
    "                        # c_pa = center_crop(pa, sha)\n",
    "\n",
    "                        test_result[f'{str(caps[0])}']['output'].append(c_output.detach().squeeze(0).cpu().numpy())\n",
    "                        # test_result[f'{str(caps[0])}']['c_padded'].append(c_pa.detach().squeeze(0).cpu().numpy())\n",
    "                        test_result[f'{str(caps[0])}']['c_padded'].append(pa.detach().squeeze(0).cpu().numpy())\n",
    "\n",
    "                        loss = criterion(c_output, pa)\n",
    "\n",
    "                        total_loss += loss\n",
    "                        total += 1\n",
    "                avg_loss = total_loss / total\n",
    "                test_text = f'Test  | Epoch: [{e+1}/{epochs}] |  MSE: {avg_loss} | Total: {total}'\n",
    "                print(test_text)\n",
    "                print('='*len(train_text))\n",
    "\n",
    "                if avg_loss < best_loss:\n",
    "                    print('Update results')\n",
    "                    best_result = test_result\n",
    "                    best_loss = avg_loss\n",
    "                    best_model = deepcopy(model.state_dict())\n",
    "\n",
    "send_tg_message(f'Finish Local Computer X8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import center_crop\n",
    "\n",
    "def save_imgs(results, root='results'):\n",
    "    \n",
    "\n",
    "    for k in results.keys():\n",
    "        # root = './resultsX4'\n",
    "        if not os.path.isdir(root):\n",
    "            os.mkdir(root)\n",
    "        \n",
    "        if not os.path.isdir(os.path.join(root, k)):\n",
    "            os.mkdir(os.path.join(root, k.rstrip()))\n",
    "        save_path = os.path.join(root, k.rstrip())\n",
    "        \n",
    "        org = Image.fromarray(results[k]['org_img'][0].numpy())\n",
    "        org.save(os.path.join(save_path, 'org.png'))\n",
    "        if len(results[k]['phrases'][0]) == 0:\n",
    "            print('ups')\n",
    "            continue\n",
    "        zz = 0\n",
    "        for i in range(len(results[k]['phrases'][0])):\n",
    "            save_path = os.path.join(root, k)\n",
    "            \n",
    "            output = torch.tensor(results[k]['output'][i])\n",
    "            output = Image.fromarray((np.clip(center_crop(output, results[k]['shape'][i]).permute(2,1,0).numpy(),0,1)*255).astype(np.uint8))\n",
    "            crop = results[k]['cropped'][i].permute(2,1,0)\n",
    "            detect = results[k]['detect'][i].permute(2,1,0)\n",
    "\n",
    "            crop = Image.fromarray(crop.numpy()) #Image.fromarray((np.clip(crop.numpy(),0,1)*255).astype(np.uint8))\n",
    "            detect = Image.fromarray(detect.numpy())\n",
    "\n",
    "            if not os.path.isdir(os.path.join(save_path, f'{results[k][\"phrases\"][0][i]}')):\n",
    "                zz = 1\n",
    "                os.mkdir(os.path.join(save_path, f'{results[k][\"phrases\"][0][i]}'))\n",
    "            save_path = os.path.join(save_path, f'{results[k][\"phrases\"][0][i]}')\n",
    "            if os.path.isdir(save_path):\n",
    "                output.save(os.path.join(save_path, f'output{zz}.png'))\n",
    "                crop.save(os.path.join(save_path, f'big{zz}.png'))\n",
    "                detect.save(os.path.join(save_path, f'detect{zz}.png'))\n",
    "                zz += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: 'resultsX8x2withDetection\\\\Three stuffed animals are sitting on a bed. \\\\three stuffed animals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msave_imgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresultsX\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdown_scale\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mx2withDetection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 36\u001b[0m, in \u001b[0;36msave_imgs\u001b[1;34m(results, root)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrases\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     35\u001b[0m     zz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphrases\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrases\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(save_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: 'resultsX8x2withDetection\\\\Three stuffed animals are sitting on a bed. \\\\three stuffed animals'"
     ]
    }
   ],
   "source": [
    "save_imgs(best_result, f'resultsX{down_scale}x2withDetection_1e-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three stuffed animals'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.mkdir('./resultsX8x2withDetection\\\\Three stuffed animals are sitting on a bed.\\\\three stuffed animals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=f'resultsX{down_scale}x2withDetection_1e-6.pkl', mode='rb') as f:\n",
    "    dicdic=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result['Three stuffed animals are sitting on a bed. '][\"phrases\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'resultsX{down_scale}x2withDetection_crop.pkl','wb') as fw:\n",
    "    pickle.dump(best_result, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './result_model/X8_x2withDetection_crop.pt'\n",
    "torch.save(best_model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "print(\"PSNR :\", peak_signal_noise_ratio(Test_result_img['output'][0], Test_result_img['big_image'][0]))\n",
    "print(\"SSIM :\", structural_similarity((Test_result_img['output'][0]*255).astype(np.uint8), (Test_result_img['big_image'][0]*255).astype(np.uint8), channel_axis=2,multichannel=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
