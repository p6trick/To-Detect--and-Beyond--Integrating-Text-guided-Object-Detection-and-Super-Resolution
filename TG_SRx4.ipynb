{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd1/six/team6/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.datasets import CocoCaptions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Detection.GroundingDINO.groundingdino.datasets.transforms as T\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,phase, transform):\n",
    "        self.phase = phase\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.dataset = CocoCaptions(root='./train2017', annFile='./annotations/captions_train2017.json')\n",
    "        else:\n",
    "            self.dataset = CocoCaptions(root='./val2017', annFile='./annotations/captions_val2017.json')\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        org_img, caps = self.dataset[idx]\n",
    "\n",
    "        img,_ = self.transform(org_img, None)\n",
    "        \n",
    "        random_idx = torch.randint(0,len(caps), (1,))\n",
    "        \n",
    "        return np.array(org_img), img, caps[random_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SResolution.TG_Umodels import *\n",
    "\n",
    "def select_model(down_scale, shape):\n",
    "    if down_scale == 2:\n",
    "        model = TG_UNet2(3,3, shape)\n",
    "    elif down_scale == 4:\n",
    "        model = TG_UNet4(3,3, shape)\n",
    "    elif down_scale == 8:\n",
    "        model = UNet8(3,3)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "down_scale = 2\n",
    "input_shape = (512//down_scale,512//down_scale)\n",
    "model = select_model(down_scale,input_shape)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 200\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "\n",
    "trainDataset = CustomDataset('train', transform)\n",
    "\n",
    "num_train_samples = 1000\n",
    "sample_ds = Subset(trainDataset, np.arange(num_train_samples))\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "# sample_dl = DataLoader(sample_ds, sampler=sample_sampler, batch_size=1)\n",
    "train_dataloader = DataLoader(sample_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "valDataset = CustomDataset('val', transform)\n",
    "num_train_samples = 200\n",
    "sample_ds = Subset(valDataset, np.arange(num_train_samples))\n",
    "sample_sampler = RandomSampler(sample_ds)\n",
    "val_dataloader = DataLoader(sample_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 443, 640, 3]), torch.Size([1, 3, 800, 1155]), tuple)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo, a, c = next(iter(train_dataloader))\n",
    "\n",
    "oo.shape, a.shape, type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Train:   0%|          | 0/1000 [00:00<?, ?it/s]FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "Train:   3%|â–Ž         | 27/1000 [00:17<07:29,  2.16it/s] "
     ]
    }
   ],
   "source": [
    "from Detection.GroundingDINO.groundingdino.util.inference import load_model, load_image_6, predict, annotate\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from torchvision.ops import box_convert\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "iterable = range(epochs)\n",
    "\n",
    "dect_model = load_model(\"./Detection/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"./Detection/GroundingDINO//weights/groundingdino_swint_ogc.pth\")\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "target_size = 512\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "best_loss = 999\n",
    "best_result = None\n",
    "for e in iterable:\n",
    "    for p in ['Train', 'Test']:\n",
    "        if p == 'Train':\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            model.train()\n",
    "\n",
    "            train_result = {}\n",
    "            \n",
    "            for oo, bimg, caps in tqdm(train_dataloader, desc='Train', leave=False):\n",
    "                # print(f'{str(caps[0])}')\n",
    "                train_result[f'{str(caps[0])}'] = {\n",
    "                    'phrases': [],\n",
    "                    'org_img':[],\n",
    "                    'shape':[],\n",
    "                    'cropped':[],\n",
    "                    'padded':[],\n",
    "                    'resized':[],\n",
    "                    'output':[],\n",
    "                    'c_padded':[]\n",
    "                }\n",
    "                \n",
    "                i_caps = str(caps[0])\n",
    "                bimg = bimg.squeeze()\n",
    "                b_boxes, b_logits, b_phrases, b_encoded_text = predict(\n",
    "                        model=dect_model,\n",
    "                        image=bimg,\n",
    "                        caption=i_caps,\n",
    "                        box_threshold=BOX_TRESHOLD,\n",
    "                        text_threshold=TEXT_TRESHOLD\n",
    "                    )\n",
    "                train_result[f'{str(caps[0])}']['phrases'].append(b_phrases)\n",
    "                oo = oo.squeeze()\n",
    "                h, w, c = oo.shape\n",
    "                boxes2 = b_boxes * torch.Tensor([w, h, w, h])\n",
    "                xyxy = box_convert(boxes=boxes2, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "                \n",
    "                for xyxy_idx in range(len(xyxy)):\n",
    "                    x1, y1, x2, y2 = xyxy[xyxy_idx].astype(int)\n",
    "                    cropped_img = oo[y1:y2, x1:x2].permute(2,1,0)\n",
    "\n",
    "                    cc, hh, ww = cropped_img.shape\n",
    "                    train_result[f'{str(caps[0])}']['shape'].append((hh,ww))\n",
    "                    nh, nw = target_size - hh, target_size - ww\n",
    "\n",
    "                    train_result[f'{str(caps[0])}']['cropped'].append(cropped_img)\n",
    "                    padd = (nw//2, nw//2, nh//2, nh//2)\n",
    "                    pimg = torch.nn.functional.pad(cropped_img, padd)\n",
    "                    pimg = resize(pimg, (target_size, target_size))\n",
    "                    train_result[f'{str(caps[0])}']['padded'].append(pimg)\n",
    "                \n",
    "                for b in train_result[f'{str(caps[0])}']['padded']:\n",
    "                    _, h, w = b.shape\n",
    "                    s_img = resize(b, (h//down_scale, w//down_scale))\n",
    "                    train_result[f'{str(caps[0])}']['resized'].append(s_img)\n",
    "\n",
    "                txt = b_encoded_text['encoded_text']\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                for re, pa, sha in zip(train_result[f'{str(caps[0])}']['resized'], train_result[str(caps[0])]['padded'],train_result[str(caps[0])]['shape'] ):\n",
    "                    re, pa = re/255., pa/255.\n",
    "                    re = re.unsqueeze(0)\n",
    "                    pa = pa.unsqueeze(0)\n",
    "                    re = re.to(device)\n",
    "                    pa = pa.to(device)\n",
    "\n",
    "                    output = model(re,txt)\n",
    "\n",
    "                    c_output = center_crop(output, sha)\n",
    "                    c_pa = center_crop(pa, sha)\n",
    "\n",
    "                    train_result[f'{str(caps[0])}']['output'].append(c_output.detach().squeeze(0).cpu().numpy())\n",
    "                    train_result[f'{str(caps[0])}']['c_padded'].append(c_pa.detach().squeeze(0).cpu().numpy())\n",
    "\n",
    "                    loss = criterion(c_output, c_pa)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss\n",
    "                    total += 1\n",
    "            avg_loss = total_loss / total\n",
    "            train_text = f'Train | Epoch: [{e+1}/{epochs}] |  MSE: {avg_loss} | Total: {total}'\n",
    "            print('='*len(train_text))\n",
    "            print(train_text)\n",
    "\n",
    "        else:\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "\n",
    "            test_result = {}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for oo, bimg, caps in tqdm(val_dataloader, desc='Test', leave=False):\n",
    "\n",
    "                    test_result[f'{str(caps[0])}'] = {\n",
    "                        'phrases': [],\n",
    "                        'org_img':[],\n",
    "                        'shape':[],\n",
    "                        'cropped':[],\n",
    "                        'padded':[],\n",
    "                        'resized':[],\n",
    "                        'output':[],\n",
    "                        'c_padded':[],\n",
    "                        'detect':[]\n",
    "                    }\n",
    "\n",
    "                    i_caps = str(caps[0])\n",
    "                    bimg = bimg.squeeze()\n",
    "                    b_boxes, b_logits, b_phrases, b_encoded_text = predict(\n",
    "                            model=dect_model,\n",
    "                            image=bimg,\n",
    "                            caption=i_caps,\n",
    "                            box_threshold=BOX_TRESHOLD,\n",
    "                            text_threshold=TEXT_TRESHOLD\n",
    "                        )\n",
    "                    test_result[f'{str(caps[0])}']['phrases'].append(b_phrases)\n",
    "                    oo = oo.squeeze()\n",
    "                    h, w, c = oo.shape\n",
    "                    boxes2 = b_boxes * torch.Tensor([w, h, w, h])\n",
    "                    xyxy = box_convert(boxes=boxes2, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "                    test_result[f'{str(caps[0])}']['org_img'].append(oo)\n",
    "                    for xyxy_idx in range(len(xyxy)):\n",
    "                        x1, y1, x2, y2 = xyxy[xyxy_idx].astype(int)\n",
    "                        cropped_img = oo[y1:y2, x1:x2].permute(2,1,0)\n",
    "\n",
    "                        cc, hh, ww = cropped_img.shape\n",
    "                        \n",
    "                        test_result[f'{str(caps[0])}']['shape'].append((hh,ww))\n",
    "                        nh, nw = target_size - hh, target_size - ww\n",
    "\n",
    "                        test_result[f'{str(caps[0])}']['cropped'].append(cropped_img)\n",
    "                        padd = (nw//2, nw//2, nh//2, nh//2)\n",
    "                        pimg = torch.nn.functional.pad(cropped_img, padd)\n",
    "                        \n",
    "                        pimg = resize(pimg, (target_size, target_size))\n",
    "                        test_result[f'{str(caps[0])}']['padded'].append(pimg)\n",
    "\n",
    "                        test_result[f'{str(caps[0])}']['detect'].append(resize(cropped_img, (hh//down_scale, ww//down_scale)))\n",
    "\n",
    "                    for b in test_result[f'{str(caps[0])}']['padded']:\n",
    "                        _, h, w = b.shape\n",
    "                        s_img = resize(b, (h//down_scale, w//down_scale))\n",
    "                        \n",
    "                        test_result[f'{str(caps[0])}']['resized'].append(s_img)\n",
    "\n",
    "                    txt = b_encoded_text['encoded_text']\n",
    "\n",
    "                    for re, pa, sha in zip(test_result[f'{str(caps[0])}']['resized'], test_result[str(caps[0])]['padded'], test_result[str(caps[0])]['shape']):\n",
    "                        re, pa = re/255., pa/255.\n",
    "                        re = re.unsqueeze(0)\n",
    "                        pa = pa.unsqueeze(0)\n",
    "                        re = re.to(device)\n",
    "                        pa = pa.to(device)\n",
    "\n",
    "                        output = model(re,txt)\n",
    "\n",
    "                        c_output = center_crop(output, sha)\n",
    "                        c_pa = center_crop(pa, sha)\n",
    "\n",
    "                        test_result[f'{str(caps[0])}']['output'].append(c_output.detach().squeeze(0).cpu().numpy())\n",
    "                        test_result[f'{str(caps[0])}']['c_padded'].append(c_pa.detach().squeeze(0).cpu().numpy())\n",
    "\n",
    "                        loss = criterion(output, pa)\n",
    "\n",
    "                        total_loss += loss\n",
    "                        total += 1\n",
    "                avg_loss = total_loss / total\n",
    "                test_text = f'Test  | Epoch: [{e+1}/{epochs}] |  MSE: {avg_loss} | Total: {total}'\n",
    "                print(test_text)\n",
    "                print('='*len(train_text))\n",
    "\n",
    "                if avg_loss < best_loss:\n",
    "                    print('Update results')\n",
    "                    best_result = test_result\n",
    "                    best_loss = avg_loss\n",
    "                    best_model = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import center_crop\n",
    "\n",
    "def save_imgs(results, root='results'):\n",
    "    \n",
    "\n",
    "    for k in results.keys():\n",
    "        # root = './resultsX4'\n",
    "        if not os.path.isdir(root):\n",
    "            os.mkdir(root)\n",
    "\n",
    "        if not os.path.isdir(os.path.join(root, k)):\n",
    "            os.mkdir(os.path.join(root, k))\n",
    "        save_path = os.path.join(root, k)\n",
    "        \n",
    "        org = Image.fromarray(results[k]['org_img'][0].numpy())\n",
    "        org.save(os.path.join(save_path, 'org.png'))\n",
    "        if len(results[k]['phrases'][0]) == 0:\n",
    "            print('ups')\n",
    "            continue\n",
    "        zz = 0\n",
    "        for i in range(len(results[k]['phrases'][0])):\n",
    "            save_path = os.path.join(root, k)\n",
    "            \n",
    "            output = torch.tensor(results[k]['output'][i])\n",
    "            output = Image.fromarray((np.clip(center_crop(output, results[k]['shape'][i]).permute(2,1,0).numpy(),0,1)*255).astype(np.uint8))\n",
    "            crop = results[k]['cropped'][i].permute(2,1,0)\n",
    "            detect = results[k]['detect'][i].permute(2,1,0)\n",
    "\n",
    "            crop = Image.fromarray(crop.numpy()) #Image.fromarray((np.clip(crop.numpy(),0,1)*255).astype(np.uint8))\n",
    "            detect = Image.fromarray(detect.numpy())\n",
    "\n",
    "            if not os.path.isdir(os.path.join(save_path, f'{results[k][\"phrases\"][0][i]}')):\n",
    "                zz = 1\n",
    "                os.mkdir(os.path.join(save_path, f'{results[k][\"phrases\"][0][i]}'))\n",
    "            save_path = os.path.join(save_path, f'{results[k][\"phrases\"][0][i]}')\n",
    "            if os.path.isdir(save_path):\n",
    "                output.save(os.path.join(save_path, f'output{zz}.png'))\n",
    "                crop.save(os.path.join(save_path, f'big{zz}.png'))\n",
    "                detect.save(os.path.join(save_path, f'detect{zz}.png'))\n",
    "                zz += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ups\n"
     ]
    }
   ],
   "source": [
    "save_imgs(best_result, f'resultsX{down_scale}withDetection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "print(\"PSNR :\", peak_signal_noise_ratio(Test_result_img['output'][0], Test_result_img['big_image'][0]))\n",
    "print(\"SSIM :\", structural_similarity((Test_result_img['output'][0]*255).astype(np.uint8), (Test_result_img['big_image'][0]*255).astype(np.uint8), channel_axis=2,multichannel=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
